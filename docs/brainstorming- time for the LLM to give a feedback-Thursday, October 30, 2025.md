# Brainstorming Session Results

**Session Date:** Thursday, October 30, 2025
**Facilitator:** Business Analyst
**Participant:** BIP

## Executive Summary

**Topic:** How long it should take for the LLM to give the feedback for the CV or make the cover letter.

**Session Goals:** To find the optimal balance between speed and depth in LLM feedback to ensure user satisfaction and high-quality, complete responses.

**Techniques Used:** Five Whys, Question Storming, SCAMPER Method

**Total Ideas Generated:** Numerous ideas across three techniques.

### Key Themes Identified:

The primary themes identified revolve around the critical trade-off between the speed of LLM feedback and the quality/completeness of that feedback. User expectations for rapid responses, especially in time-sensitive scenarios like job applications, clash with the technical limitations of LLMs, where speed can compromise reasoning depth and thoroughness. The session highlighted the need to balance user patience with the delivery of valuable, actionable insights to maintain engagement and trust.

## Technique Sessions

### Five Whys

**Initial Problem:** The duration of LLM feedback for CVs or cover letters.

1.  **Why is duration a concern?** If too long, users abandon; if too quick, answers are bad/incomplete.
2.  **Why do users abandon if too long?** Limited time, desire for quick results.
3.  **Why limited time/quick results?** Busy lives, time-sensitive needs (job interviews), seeking inspiration.
4.  **Why critical for quick delivery in time-sensitive situations?** Users will use other apps or give bad reviews.
5.  **Why quick leads to bad/incomplete answers?** Not thought through, lines skipped.
6.  **Why not thought through/lines skipped?** System reduces reasoning steps, uses lower precision, explores fewer continuations, leading to shallower, less deliberate reasoning.

### Question Storming

1.  How to measure feedback quality?
2.  How does input complexity affect processing time?
3.  Do different feedback types require different speeds?
4.  Do users prefer quick summaries before detailed insights?
5.  How does waiting time influence user trust and satisfaction with the LLM’s feedback?

### SCAMPER Method

#### Substitute

*   Full initial analysis with a faster preliminary scan that highlights key issues first. The detailed feedback could then follow in a second step.
*   Slow reasoning processes with pre-trained templates or cached responses for common CV patterns, ensuring users receive quick yet meaningful insights.

#### Combine

*   A quick initial scan with user-selected preferences for speed or depth, allowing the system to tailor its feedback mode dynamically.
*   Automated scoring with personalized suggestions, so users get both instant metrics and thoughtful guidance.
*   Real-time streaming feedback with a later, more detailed review to enhance both responsiveness and depth.

#### Adapt

*   Techniques from streaming services that adjust quality based on connection speed, applying similar logic to balance feedback depth with response time.
*   Real-time gaming systems that manage latency to inspire adaptive reasoning models.
*   Search engines’ quick previews to deliver instant, partial feedback before the full analysis.

#### Modify

*   The LLM’s prompt structure to prioritize essential feedback first, ensuring the most important points appear quickly.
*   The feedback layout to highlight critical issues upfront, with detailed explanations revealed afterward.
*   Response generation settings—like reasoning depth or token limits—to fine-tune the balance between speed and completeness.

#### Put to Other Uses

*   The LLM’s pattern recognition to pre-categorize CVs based on structure or profession, allowing faster, more targeted feedback.
*   The LLM’s understanding of job descriptions to auto-match CVs with role requirements, improving relevance without extra delay.
*   Previous feedback data to train quick-response models for common issues, speeding up initial evaluations.

#### Eliminate

*   Unnecessary introductory text and overly detailed explanations that slow down delivery without adding real value.
*   Redundant processing steps, such as reanalyzing already-identified patterns, to streamline response time.
*   Nonessential formatting or decorative elements in the feedback to help the system focus purely on clarity and actionable insights.

#### Reverse

*   The process by having the LLM ask clarifying questions before giving feedback, ensuring precision and reducing wasted reasoning.
*   Let users set their preferred feedback duration first, so the model tailors its depth and detail accordingly.
*   The order of output—starting with quick, high-level insights and revealing deeper analysis only if the user requests it, keeping the experience both fast and flexible.

## Idea Categorization

### Immediate Opportunities

*   Implement a faster preliminary scan.
*   Prioritize essential feedback in prompt structure.
*   Adjust feedback layout to highlight critical issues upfront.
*   Eliminate unnecessary introductory text and redundant processing steps.

### Future Innovations

*   Develop adaptive reasoning models inspired by real-time gaming.
*   Train quick-response models using previous feedback data.
*   Implement user-selected preferences for speed or depth.

### Moonshots

*   LLM asks clarifying questions before generating any feedback.
*   Real-time streaming feedback with a later, more detailed review.

### Insights and Learnings

The session reinforced that the optimal feedback duration is a dynamic balance influenced by user context, expectations, and the technical capabilities of the LLM. Sacrificing speed for quality, or vice-versa, leads to negative user experiences. The key lies in providing flexible, multi-stage feedback that caters to immediate needs while offering depth on demand.

## Action Planning

(Not explicitly captured in this session)

## Reflection and Follow-up

(Not explicitly captured in this session)

---

_Session facilitated using the BMAD CIS brainstorming framework_
